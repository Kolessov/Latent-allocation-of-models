{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Knowledge_distillation_MNIST.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AKu5xAWISTXH"
      },
      "source": [
        "Knowledge Distillation with Pytorch on MNIST\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZrpZ5X_w-oB2"
      },
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tqdm\n",
        "from tqdm import tqdm_notebook\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r5WpaCxayx3T"
      },
      "source": [
        "torch.cuda.is_available()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q6G7heJ3Smqw"
      },
      "source": [
        "There are teacher neural network  and student neural network here. Each network constitutes convolutional neural networks that is composed of 2 layers and one linear layer, whose main goal is to obtain logits. The distinguishing of these networks is that student neura network has less filters(kernels) in each layer unless teacher  neural network. The main goal is to train student network in oeder to get the same predictions as teacher network."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oCUJuJoKESxu"
      },
      "source": [
        "class Teacher(torch.nn.Module): \n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.teacher_net = torch.nn.Sequential(\n",
        "        torch.nn.Conv2d(in_channels = 1 ,out_channels = 256, kernel_size = 3 ,stride = 2, padding = 1 ,bias = True),\n",
        "        torch.nn.BatchNorm2d(256),\n",
        "        torch.nn.LeakyReLU(0.2),\n",
        "        \n",
        "\n",
        "        torch.nn.MaxPool2d(kernel_size = 2, stride = 1 , padding =1),\n",
        "\n",
        "        torch.nn.Conv2d(in_channels = 256 , out_channels = 512 ,kernel_size = 3, stride = 2, padding = 1),\n",
        "        torch.nn.BatchNorm2d(512)\n",
        "        \n",
        "    )\n",
        "\n",
        "    self.fc = torch.nn.Sequential( \n",
        "        torch.nn.Linear(in_features = 512*8*8  , out_features = 300),\n",
        "        torch.nn.LeakyReLU(0.2),\n",
        "        torch.nn.Linear(in_features = 300, out_features = 10),\n",
        "        torch.nn.Dropout(0.2)\n",
        "    )\n",
        "\n",
        "  def forward(self,input):\n",
        "    input = self.teacher_net(input)\n",
        "    input = input.view(-1, 512*8*8)\n",
        "    return torch.nn.functional.relu(self.fc(input)) # return [batch, 10] i.e batch of logits for each class"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MRQovqEuIYqo"
      },
      "source": [
        "class Student(torch.nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.student_net = torch.nn.Sequential(\n",
        "      torch.nn.Conv2d(in_channels = 1 ,out_channels = 16, kernel_size = 3 ,stride = 2, padding = 1 ,bias = True),\n",
        "      torch.nn.BatchNorm2d(16),\n",
        "      torch.nn.LeakyReLU(0.2),\n",
        "\n",
        "      torch.nn.MaxPool2d(kernel_size = 2, stride = 1 , padding =1),\n",
        "\n",
        "      torch.nn.Conv2d(in_channels = 16 , out_channels = 32 ,kernel_size = 3, stride = 2, padding = 1),\n",
        "      torch.nn.BatchNorm2d(32)\n",
        "    )\n",
        "    self.fc = torch.nn.Sequential(\n",
        "        torch.nn.Linear(in_features = 32*8*8 , out_features= 100),\n",
        "        torch.nn.Linear(in_features = 100 ,out_features = 10),\n",
        "        torch.nn.Dropout(0.2)\n",
        "\n",
        "    )\n",
        "\n",
        "    \n",
        "  def forward(self,input):\n",
        "    input = self.student_net(input)\n",
        "    input = input.view(-1,32*8*8)\n",
        "    return torch.nn.functional.relu(self.fc(input)) # return [batch,10] i.e batch of logits for each class"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HvGHpKLz-5q_"
      },
      "source": [
        "class Distiller(torch.nn.Module):\n",
        "  \n",
        "  def __init__(self, student, teacher,optimizer,student_loss_fn,distillation_loss_fn,\n",
        "               alpha,temperature):\n",
        "    \n",
        "    super().__init__()\n",
        "    self.teacher = teacher\n",
        "    self.student = student\n",
        "    self.optimizer = optimizer\n",
        "    self.student_loss_fn = student_loss_fn\n",
        "    self.distillation_loss_fn = distillation_loss_fn\n",
        "    self.alpha = alpha\n",
        "    self.temperature = temperature\n",
        "  \n",
        "  \n",
        "  # one train step \n",
        "  def train_step(self, x,y):\n",
        "    \n",
        "    self.optimizer.zero_grad()\n",
        "    teacher_predictions = self.teacher(x).detach()\n",
        "    # since we don't want train teacher model\n",
        "    \n",
        "    # x : torch.Size([batch,1,28,28])\n",
        "    # y: torch.Size ([batch])\n",
        "    student_predictions = self.student(x)\n",
        "    # student_predictions : torch.Size([batch,10])\n",
        "\n",
        "     \n",
        "\n",
        "    distillation_loss = self.distillation_loss_fn(torch.nn.Softmax(dim=1)(student_predictions/self.temperature),\n",
        "                                                  torch.nn.Softmax(dim=1)(teacher_predictions/self.temperature)).sum(1).mean()\n",
        "    # distillation_loss : torch.Size([])\n",
        "    \n",
        "    student_loss = self.student_loss_fn(student_predictions, y.long().cuda()) #  loss of the ground-truth labels\n",
        "\n",
        "    \n",
        "    student_loss =  ( 1 - self.alpha )*distillation_loss +  self.alpha*student_loss\n",
        "    student_loss.backward()\n",
        "    self.optimizer.step()\n",
        "\n",
        "    return student_loss.item() \n",
        "\n",
        "\n",
        "\n",
        "  def test_step(self, x,y):\n",
        "    student_predictions = self.student(x).detach()\n",
        "    student_loss = self.student_loss_fn(student_predictions,y.long().cuda()).detach().cpu().numpy()\n",
        "\n",
        "    return student_loss\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9r5KO_TXW_sY"
      },
      "source": [
        "Then, one can prepare the dataset for training of the student network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cvkvBc0wW4zx"
      },
      "source": [
        "BATCH_SIZE = 100\n",
        "transform = torchvision.transforms.Compose([torchvision.transforms.ToTensor(),torchvision.transforms.Normalize(mean = 0.5, std = 0.5)])\n",
        "train_data = torchvision.datasets.MNIST(root='./data',train=True,     transform=transform,  download=True)\n",
        "test_data =  torchvision.datasets.MNIST(root='./data' ,train =False, transform=transform ,download =True)\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_data,batch_size=BATCH_SIZE,shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(dataset=test_data,batch_size=BATCH_SIZE,shuffle = False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o7bBfSkcaTZH"
      },
      "source": [
        "The following code is aimed to train teacher. Since , we are inclined to believe that teacher is fixed while stident is training .So, let's train the teacher\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o2nDaPe7bTOX"
      },
      "source": [
        "teacher = Teacher().cuda()\n",
        "optimizer_teacher = torch.optim.Adam(teacher.parameters(),lr=1e-4)\n",
        "criterion = torch.nn.CrossEntropyLoss()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qqiq6vhVcZBH"
      },
      "source": [
        "def train_epoch(model,optimizer,train_loader):\n",
        "\n",
        "  losses = list()\n",
        "  model.train()\n",
        "\n",
        "  for data,label in tqdm_notebook(train_loader):\n",
        "    data,label = torch.autograd.Variable(data),torch.autograd.Variable(label)\n",
        "    optimizer.zero_grad()\n",
        "    data,label = data.cuda(),label.cuda()\n",
        "\n",
        "    loss = criterion( model(data) , label)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    losses.append(loss.item())\n",
        "  return losses\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cxFct6TOdckb"
      },
      "source": [
        "def test_model(model,test_loader):\n",
        "\n",
        "  model.eval()\n",
        "  total_loss = 0\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for data,label in test_loader:\n",
        "      data,label = torch.autograd.Variable(data),torch.autograd.Variable(label)\n",
        "      data,label = data.cuda(),label.cuda()\n",
        "\n",
        "      #indices_of_max = torch.max(probs,dim=1)[1]\n",
        "      loss = criterion( model(data),label)\n",
        "      total_loss += loss.item()*data.shape[0]\n",
        "    avg_loss = total_loss/(len(test_loader.dataset))\n",
        "  return avg_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-batNRpxqTKX"
      },
      "source": [
        "def train_test_model(model, train_loader,test_loader,optimizer, num_epochs):\n",
        "  train_loss = list()\n",
        "  test_loss = list()\n",
        "\n",
        "  for epoch in tqdm_notebook(range(num_epochs)):\n",
        "\n",
        "    train_loss.extend( train_epoch(model,optimizer,train_loader) )\n",
        "    test_loss.append( test_model(model,test_loader) )\n",
        "  \n",
        "  return train_loss , test_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PLlfcclMwZO8"
      },
      "source": [
        "train_loss,test_loss = train_test_model(teacher,train_loader,test_loader,optimizer_teacher,5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eP4ooUS80_-Y"
      },
      "source": [
        "fig,ax = plt.subplots(1,2,figsize=(10,2))\n",
        "ax = ax.flatten()\n",
        "ax[0].plot(train_loss,label = 'train loss')\n",
        "ax[1].plot(test_loss, label = 'test loss')\n",
        "ax[0].legend()\n",
        "ax[0].set_xlabel(\"num iterations\")\n",
        "ax[1].set_xlabel(\"num epochs\")\n",
        "ax[1].legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V6xpVCp1CHt5"
      },
      "source": [
        "As soon as we trained the teacher model, one can move on to distillation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PulWc0Hbes2p"
      },
      "source": [
        "student = Student().cuda()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QpFbDG-VCOkW"
      },
      "source": [
        "distiller = Distiller(student, teacher, optimizer=torch.optim.Adam(student.parameters(),lr=1e-3),\n",
        "                 student_loss_fn = torch.nn.CrossEntropyLoss(),distillation_loss_fn = torch.nn.KLDivLoss(reduction='none'),\n",
        "                 alpha = 0.1,temperature = 10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WHW-nZrvoElX"
      },
      "source": [
        "Train student model via the teacher model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cz8UHg4PqZ0C"
      },
      "source": [
        "def train_student(num_epochs,distiller,train_loader):\n",
        "  losses_of_student_train = list()\n",
        "  losses_of_student_test =  list()\n",
        "\n",
        "  for epoch in tqdm_notebook(range(num_epochs)):\n",
        "    distiller.student.train()\n",
        "     \n",
        "    for data,label in train_loader:\n",
        "      data  = torch.autograd.Variable(data)\n",
        "      label = torch.autograd.Variable(label)\n",
        "      data = data.cuda()\n",
        "      label = label.cuda()\n",
        "      loss_of_step = distiller.train_step(data,label)\n",
        "      losses_of_student_train.append(loss_of_step)\n",
        "\n",
        "    total_test = 0\n",
        "    with torch.no_grad():\n",
        "      for data,label in test_loader:\n",
        "        data = torch.autograd.Variable(data)\n",
        "        label = torch.autograd.Variable(label)\n",
        "        data = data.cuda()\n",
        "        label = label.cuda()\n",
        "        total_test += np.sum(distiller.test_step(data,label))\n",
        "      avg_loss = total_test/ (len(test_loader))\n",
        "    losses_of_student_test.append(avg_loss)\n",
        "  \n",
        "\n",
        "  return losses_of_student_train, losses_of_student_test\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W8UYa3dJv1sJ"
      },
      "source": [
        "losses_of_student_train, losses_of_student_test = train_student(5,distiller,train_loader)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BsQILnwkyurI"
      },
      "source": [
        "fig,ax = plt.subplots(1,2,figsize=(10,2))\n",
        "ax = ax.flatten()\n",
        "ax[0].plot(losses_of_student_train,label = 'train loss')\n",
        "ax[1].plot(losses_of_student_test, label = 'test loss')\n",
        "ax[0].legend()\n",
        "ax[0].set_xlabel(\"num iterations\")\n",
        "ax[1].set_xlabel(\"num epochs\")\n",
        "ax[1].legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ps8EP6mqBzAP"
      },
      "source": [
        "https://keras.io/examples/vision/knowledge_distillation/"
      ]
    }
  ]
}